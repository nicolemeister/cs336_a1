Initializing vocabulary with special tokens...
Adding byte vocabulary...
Attempting to split 2124.55MB file into 16 chunks
Reading input text and finding chunk boundaries...
Input path:  /home/c-nmeist/cs336_a1/data/TinyStoriesV2-GPT4-train.txt
Found 16 valid chunk boundaries [0, 139234745, 278469249, 417704387, 556938507, 696173730, 835407773, 974642339, 1113876594, 1253112657, 1392345974, 1531581005, 1670815354, 1810050205, 1949284164, 2088518780, 2227753162]
Processing chunks with multiprocessing pool...
Using 16 processes
